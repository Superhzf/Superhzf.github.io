---
title: 'Mathematics behind linear SVM'
date: 2020-09-09
permalink: /posts/2020/09/blog-post-1/
tags:
  - SVM
  - machine learning
---

## 1. Introduction
About ten years ago, Support Vector Machine (SVM) was as hot as what deep neural
networks are like today. It was widely used in the areas like computer vision
and natural language processing. Although machine learning research has been
dominated by deep learning, SVM is still worth the time to study because SVM is
a good application of convex optimization which is widely used in other
algorithms and everyday life. Therefore, questions related to SVM are often asked
in the interviews for the machine learning positions to test the basic math skills
of the candidates. On the top of that, SVM is simple and it works well when the
sample size is small. In this article, I will try to derive the math details
behind linear SVM. This article is for those who are interested in the math details of SVM.

## 2. Binary classification
Given a set of data points {($\mathbf{x_1}$,$y_1$), ($\mathbf{x_2}$,$y_2$),...,($\mathbf{x_m}$,$y_m$)}, where $\mathbf{x_i}$ $\in$ $\mathbb{R}^d$ and $y$ $\in$ {-1,1}, the target of the binary classification task is to learn a function from the dataset $h$: $\mathbb{R}$ $\rightarrow$ {-1,1}, such that $h(\mathbf{x_i})$ = $y_i$, that is

$$h(\mathbf{x_i})=\begin{cases}
1 & y_i = 1;\\
-1 & y_i = -1.
\end{cases}\tag{1}
$$

A simpler form is:
$$\forall i,y_ih(\mathbf{x_i})=1\tag{2}$$

Furthermore, we assume that the format of the function is the linear combination of features $\mathbf{x_i}$, that is:
$$h(\mathbf{x_i})=sign(\mathbf{w^\top}\mathbf{x_i}+b)\tag{3}$$
where $\mathbf{w_i} \in \mathbb{R}^d, b \in \mathbb{R}$

Therefore, the target of the binary classification problem is to find a set of parameters $(\mathbf{w}, b)$ such that
$$\forall i,y_i(\mathbf{w^\top}\mathbf{x_i}+b)>0\tag{4}$$
In other words, we hope to find a classification super plane that can classify samples.

## 3. Support Vector Machine
Linear SVM is one of the binary classification models which means we have to find a set of $(\mathbf{w}, b)$ that fulfill the equation (4).

However, the problem is that there exists more than one set of possible parameters, which of them is the best? One intuitive solution is that the plane which is far from both of the two classes is supposed to be the best one because it is robust to the random disturbance and is able to correctly classify samples that are close to it. As a result of it, this plan is not easy to be overfitted.

### 3.1 Margin
Before defining margin, I will revisit how to calculate the distance between a point to a plane.

**Lemma 1**: The distance between point $\mathbf{p}$, which is in space $\mathbb{R^d}$, and super plane $\mathbf{w^\top}\mathbf{x}+b=0$ is
$$\frac{1}{||\mathbf{w}||}|\mathbf{w^\top}\mathbf{x}+b|\tag{5}$$

Proof: Suppose $\mathbf{x_1},\mathbf{x_2}$ are two points in the super plane, then

$\mathbf{w^\top}(\mathbf{x_1}-\mathbf{x_2})$ = $\mathbf{w^\top}\mathbf{x_1}-\mathbf{w^\top}\mathbf{x_2}$ = (-b) - (-b) = 0

So, $\mathbf{w}\bot(\mathbf{x_1}-\mathbf{x_2})$. Since $\mathbf{x_1}-\mathbf{x_2}$ is parallel to the super plane, so $\mathbf{w}$ is vertical to the super plane. Next, suppose $\mathbf{x}$ is a random point in the super plane, then the distance between point $\mathbf{p}$ and the super plane is the projection of the line between $\mathbf{p}$ and $\mathbf{x}$ to $\mathbf{w}$:
$$\begin{align}
distance &= proj_w(\mathbf{p}-\mathbf{x})\\
&=||\mathbf{p}-\mathbf{x}||\cdot|cos(\mathbf{w},\mathbf{p}-\mathbf{x})|\\
&=||\mathbf{p}-\mathbf{x}||\cdot\frac{\mathbf{w^\top}(\mathbf{p}-\mathbf{x})}{||\mathbf{w}||\cdot||\mathbf{p}-\mathbf{x}||}\\
&=\frac{1}{||\mathbf{w}||}|\mathbf{w^\top}\mathbf{p}-\mathbf{w^\top}\mathbf{x}|\\
&=\frac{1}{||\mathbf{w}||}|\mathbf{w^\top}\mathbf{p}+b|
\end{align}\tag{6}
$$

**Definition 1. Margin.** Margin $\gamma$ shows that two times the distance between the samples that are closest to the classification super plane and the classification super plane, that is.

$$\gamma = 2 \underset{i}{min}\frac{1}{||\mathbf{w}||}\cdot|\mathbf{w^\top}\mathbf{x_i}+b|$$

**Theorem 1.** The target of linear SVM is to find a set of parameter $(\mathbf{w},b)$ such that
$$\underset{\mathbf{w},b}{max}\underset{i}{min}\frac{2}{||\mathbf{w}||}\cdot|\mathbf{w^\top}\mathbf{x_i}+b|\tag{7}$$

$$s.t.\qquad y_i(\mathbf{w^\top}\mathbf{x_i}+b)>0,i=1,2,...,m$$

That is, the idea of SVM is to find a classification super plane classifying samples into two different classes and it is the most distant super plane that is to all the samples.

### 3.2 Transform to a convex optimization problem

In order to solve the problem described in **theorem 1**, we have to simplify it making it become a convex optimization problem.

**Definition 2.** Convex quadratic optimization problems are those whose target function is a convex quadratic function, and it subjects to linear restraints.

$$\underset{\mathbf{u}}{min}\frac{1}{2}\mathbf{u^\top}\mathbf{Qu}+\mathbf{t^\top}\mathbf{u}\tag{8}$$
$$s.t.\qquad\mathbf{c^\top_iu}\geq d_i, i=1,2,3,...,m$$

**Lemma 2.** Suppose $(\mathbf{w}, b)$ is a solution to **theorem 1**, then for any $r>0$, $(r\mathbf{w}, rb)$ is also the solution to the optimization problem.

Proof:
$$\frac{2}{||r\mathbf{w}||}|(r\mathbf{w})^\top\mathbf{x_i}+rb|=\frac{2}{||\mathbf{w}||}|\mathbf{w}^\top\mathbf{x_i}+b|\tag{9}$$
$$y_i((r\mathbf{w})^\top x_i+rb)>0 \leftrightarrow y_i(\mathbf{w}^\top x_i+b)>0\tag{10}$$

Based on the **lemma 2**, we are able to further simplify **theorem 1** by adding a constraint
$$\underset{i}{min}|\mathbf{w}^\top\mathbf{x_i}+b|=1\tag{11}$$

**Theorem 2.** The problem described in **theorem 1** is equal to finding a set of parameter ($\mathbf{w},b$) such that
$$\underset{\mathbf{w},b}{min}\frac{1}{2}\mathbf{w^\top w}\tag{12}$$
$$s.t. \qquad y_i(\mathbf{w^\top}x_i+b)\geq 1,i=1,2,...,m$$

Proof:
The target function is obvious. Thanks to the restraint (11) plus $y_i \in$ {-1,1}, the constraints in (7) become $y_i(\mathbf{w^\top}x_i+b)\geq1,i=1,2,...,m$.

Now, the problem is whether the equation holds at the optimal point ($\mathbf{w},b$). Suppose at the optimal point ($\mathbf{w},b$) the equation does not hold, which means $min_iy_i(\mathbf{w^\top}x_i+b)>1$. Then, there must exists a point $(r\mathbf{w},rb), 0<r<1$ such that $min_iy_i((r\mathbf{w})^\top x_i+rb)=1$ and $\frac{1}{2}||r\mathbf{w}||^2<\frac{1}{2}||w||^2$. Thus, ($\mathbf{w},b$) is not the optimal point which is paradoxical to the assumption.

**Theorem 3.** The linear SVM described in **theorem 2** belongs to the convex quadratic optimization problems which includes $d+1$ variables and $m$ constraints.

Proof:
