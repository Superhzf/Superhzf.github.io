---
title: 'Mathematics behind linear SVM'
date: 2020-09-09
permalink: /posts/2020/09/blog-post-1/
excerpt: "Math details of linear SVM<br/><img src='/images/svm.png'>"
tags:
  - SVM
  - Machine Learning
  - Optimization
---

## 1. Introduction
About ten years ago, Support Vector Machine (SVM) was as hot as what deep neural
networks are like today. It was widely used in the areas like computer vision
and natural language processing. Although machine learning research has been
dominated by deep learning, SVM is still worth the time to study because SVM is
a good application of convex optimization which is widely used in other
algorithms and everyday life. Therefore, questions related to SVM are often asked
in the interviews for the machine learning positions to test the basic math skills
of the candidates. On the top of that, SVM is simple and it works well when the
sample size is small. In this article, I will try to derive the math details
behind linear SVM. This article is for those who are interested in the math details of SVM.

## 2. Binary classification
Given a set of data points {($\mathbf{x_1}$,$y_1$), ($\mathbf{x_2}$,$y_2$),...,($\mathbf{x_m}$,$y_m$)}, where $\mathbf{x_i}$ $\in$ $\mathbb{R}^d$ and $y$ $\in$ {-1,1}, the target of the binary classification task is to learn a function from the dataset $h$: $\mathbb{R}$ $\rightarrow$ {-1,1}, such that $h(\mathbf{x_i})$ = $y_i$, that is

$$h(\mathbf{x_i})=\begin{cases}
1 & y_i = 1;\\
-1 & y_i = -1.
\end{cases}\tag{1}
$$

A simpler form is:
$$\forall i,y_ih(\mathbf{x_i})=1\tag{2}$$

Furthermore, we assume that the format of the function is the linear combination of features $\mathbf{x_i}$, that is:
$$h(\mathbf{x_i})=sign(\mathbf{w^\top}\mathbf{x_i}+b)\tag{3}$$
where $\mathbf{w_i} \in \mathbb{R}^d, b \in \mathbb{R}$

Therefore, the target of the binary classification problem is to find a set of parameters $(\mathbf{w}, b)$ such that
$$\forall i,y_i(\mathbf{w^\top}\mathbf{x_i}+b)>0\tag{4}$$
In other words, we hope to find a classification super plane that can classify samples.

## 3. Linear Support Vector Machine
Linear SVM is one of the binary classification models which means we have to find a set of $(\mathbf{w}, b)$ that fulfill the equation (4).

However, the problem is that there exists more than one set of possible parameters, which of them is the best? One intuitive solution is that the plane which is far from both of the two classes is supposed to be the best one because it is robust to the random disturbance and is able to correctly classify samples that are close to it. As a result of it, this plan is not easy to be overfitted.

### 3.1. Margin
Before defining margin, I will revisit how to calculate the distance between a point to a plane.

**Lemma 1**: The distance between point $\mathbf{p}$, which is in space $\mathbb{R^d}$, and super plane $\mathbf{w^\top}\mathbf{x}+b=0$ is
$$\frac{1}{||\mathbf{w}||}|\mathbf{w^\top}\mathbf{x}+b|\tag{5}$$

Proof: Suppose $\mathbf{x_1},\mathbf{x_2}$ are two points in the super plane, then

$\mathbf{w^\top}(\mathbf{x_1}-\mathbf{x_2})$ = $\mathbf{w^\top}\mathbf{x_1}-\mathbf{w^\top}\mathbf{x_2}$ = (-b) - (-b) = 0

So, $\mathbf{w}\bot(\mathbf{x_1}-\mathbf{x_2})$. Since $\mathbf{x_1}-\mathbf{x_2}$ is parallel to the super plane, so $\mathbf{w}$ is vertical to the super plane. Next, suppose $\mathbf{x}$ is a random point in the super plane, then the distance between point $\mathbf{p}$ and the super plane is the projection of the line between $\mathbf{p}$ and $\mathbf{x}$ to $\mathbf{w}$:
$$\begin{align}
distance &= proj_w(\mathbf{p}-\mathbf{x})\\
&=||\mathbf{p}-\mathbf{x}||\cdot|cos(\mathbf{w},\mathbf{p}-\mathbf{x})|\\
&=||\mathbf{p}-\mathbf{x}||\cdot\frac{\mathbf{w^\top}(\mathbf{p}-\mathbf{x})}{||\mathbf{w}||\cdot||\mathbf{p}-\mathbf{x}||}\\
&=\frac{1}{||\mathbf{w}||}|\mathbf{w^\top}\mathbf{p}-\mathbf{w^\top}\mathbf{x}|\\
&=\frac{1}{||\mathbf{w}||}|\mathbf{w^\top}\mathbf{p}+b|
\end{align}\tag{6}
$$

**Definition 1. Margin.** Margin $\gamma$ shows that two times the distance between the samples that are closest to the classification super plane and the classification super plane, that is.

$$\gamma = 2 \underset{i}{min}\frac{1}{||\mathbf{w}||}\cdot|\mathbf{w^\top}\mathbf{x_i}+b|$$

**Theorem 1.** The target of linear SVM is to find a set of parameter $(\mathbf{w},b)$ such that
$$\underset{\mathbf{w},b}{max}\underset{i}{min}\frac{2}{||\mathbf{w}||}\cdot|\mathbf{w^\top}\mathbf{x_i}+b|\tag{7}$$

$$s.t.\qquad y_i(\mathbf{w^\top}\mathbf{x_i}+b)>0,i=1,2,...,m$$

That is, the idea of SVM is to find a classification super plane classifying samples into two different classes and it is the most distant super plane that is to all the samples.

### 3.2. Transform to a convex optimization problem

In order to solve the problem described in **theorem 1**, we have to simplify it making it become a convex optimization problem.

**Definition 2.** Convex quadratic optimization problems are those whose target function is a convex quadratic function, and it subjects to linear constraints.

$$\underset{\mathbf{u}}{min}\frac{1}{2}\mathbf{u^\top}\mathbf{Qu}+\mathbf{t^\top}\mathbf{u}\tag{8}$$
$$s.t.\qquad\mathbf{c^\top_iu}\geq d_i, i=1,2,3,...,m$$

**Lemma 2.** Suppose $(\mathbf{w}, b)$ is a solution to **theorem 1**, then for any $r>0$, $(r\mathbf{w}, rb)$ is also the solution to the optimization problem.

Proof:
$$\frac{2}{||r\mathbf{w}||}|(r\mathbf{w})^\top\mathbf{x_i}+rb|=\frac{2}{||\mathbf{w}||}|\mathbf{w}^\top\mathbf{x_i}+b|\tag{9}$$
$$y_i((r\mathbf{w})^\top x_i+rb)>0 \leftrightarrow y_i(\mathbf{w}^\top x_i+b)>0\tag{10}$$

Based on the **lemma 2**, we are able to further simplify **theorem 1** by adding a constraint
$$\underset{i}{min}|\mathbf{w}^\top\mathbf{x_i}+b|=1\tag{11}$$

**Theorem 2.** The problem described in **theorem 1** is equal to finding a set of parameter ($\mathbf{w},b$) such that
$$\underset{\mathbf{w},b}{min}\frac{1}{2}\mathbf{w^\top w}\tag{12}$$
$$s.t. \qquad y_i(\mathbf{w^\top}x_i+b)\geq 1,i=1,2,...,m$$

Proof:
The target function is obvious. Thanks to the constraint (11) plus $y_i \in$ {-1,1}, the constraints in (7) become $y_i(\mathbf{w^\top}x_i+b)\geq1,i=1,2,...,m$.

Now, the problem is whether the equation holds at the optimal point ($\mathbf{w},b$). Suppose at the optimal point ($\mathbf{w},b$) the equation does not hold, which means $min_iy_i(\mathbf{w^\top}x_i+b)>1$. Then, there must exists a point $(r\mathbf{w},rb), 0<r<1$ such that $min_iy_i((r\mathbf{w})^\top x_i+rb)=1$ and $\frac{1}{2}\|r\mathbf{w}\|^2<\frac{1}{2}\|w\|^2$. Thus, ($\mathbf{w},b$) is not the optimal point which is paradoxical to the assumption.

**Theorem 3.** The linear SVM described in **theorem 2** belongs to the convex quadratic optimization problems which includes $d+1$ variables and $m$ constraints.

Proof:
Let
$$\mathbf{u}=
\left[
\begin{matrix}
\mathbf{w}\\
b
\end{matrix}
\right],
\mathbf{Q}=\left[
\begin{matrix}
\mathbf{I}&0\\
0&0
\end{matrix}
\right],
\mathbf{t}=\mathbf{0}\tag{13}$$
$$\mathbf{c_i}=y_i\left[
\begin{matrix}
\mathbf{x_i}\\
1
\end{matrix}
\right],
d_i=1\tag{14}$$

Then plug (13) and (14) into (8).

## 4. Dual Problems
After simplifying the problem making it into a convex quadratic optimization problem, it's time to
solve the problem. However, the problem can be further simplified.

### 4.1. Lagrange function and its dual format

Lagrange function is an important method to solve optimization problems with constraints.

**Defition 3.** For the optimization problem below:
$$\underset{\mathbf{u}}{min} \quad f(\mathbf{u})\tag{15}$$
$$s.t. \quad g_i(\mathbf{u})\leq0,i=1,2,...m,$$
$$\qquad \quad h_j(\mathbf{u})=0,j=1,2,...,n$$
The definition of (15) Lagrange function is
$$\mathcal{L}(\mathbf{u,\alpha,\beta})=f(\mathbf{u})+\sum_{i=1}^m\alpha_ig_i(\mathbf{u})+\sum_{j=1}^n\beta_ih_j(\mathbf{u})\tag{16}$$
where $\alpha_i\geq0$

**Lemma 3.** The problem defined in (15) is equal to
$$\underset{u}{min}\underset{\alpha,\beta}{max}\quad\mathcal{L}(\mathbf{u,\alpha,\beta})\tag{17}$$
$$s.t. \quad \alpha_i \geq 0,i=1,2,...,m$$

Proof:
$$\begin{align}
&\quad\underset{\mathbf{u}}{min}\underset{\mathbf{\alpha, \beta}}{max}\quad \mathcal{L}\mathbf(u,\alpha,\beta)\\
&=\underset{\mathbf{u}}{min}(f(\mathbf{u})+\underset{\mathbf{\alpha,\beta}}{max}(\sum_{i=1}^m\alpha_ig_i(\mathbf{u})+\sum_{j=1}^n\beta_ih_j(\mathbf{u})))\\
\end{align}$$

If $\mathbf{u}$ follows the constraints in (15), then $g_i(\mathbf{u})\leq0$ and $h_j(\mathbf{u})=0$, then we get:

$$\begin{align}
&\quad\underset{\mathbf{u}}{min}\underset{\mathbf{\alpha, \beta}}{max}\quad \mathcal{L}\mathbf(u,\alpha,\beta)\\
&=\underset{\mathbf{u}}{min}f(\mathbf{u})
\end{align}$$

**Definition 4.** KKT condition: **The optimal solution** of (17) has to fulfill the following conditions:
* $g_i(\mathbf{u})\leq0,h_i(\mathbf{u})=0$
* $\alpha_i\geq0$
* complementary slackness: $\alpha_ig_i(\mathbf{u})=0$

Proof:
The condition 1 and condition 2 are the constraints of **lemma 3.** Besides, at the optimal point, $\alpha_ig_i(\mathbf{u})=0$

**Definition 5.** Dual problem: The dual problem of (17) is

$$\underset{\alpha,\beta}{max}\underset{\mathbf{u}}{min}\quad \mathcal{L}(\mathbf{u,\alpha,\beta})\tag{18}$$
$$s.t. \quad \alpha_i\geq0,i=1,2,...,m$$

(17) is called the primal problem.

**Lemma 4.** The dual problem is the lower boundary of the primal problem:

$$\underset{\alpha,\beta}{max}\underset{\mathbf{u}}{min}\quad \mathcal{L}(\mathbf{u,\alpha,\beta})
\leq
\underset{\mathbf{u}}{min}\underset{\alpha,\beta}{max}\quad \mathcal{L}(\mathbf{u,\alpha,\beta})$$

Proof:
For any ($\alpha',\beta'$), $min_u\mathcal{L}(\mathbf{u,\alpha',\beta'}) \leq min_umax_{\alpha,\beta}\mathcal{L}(\mathbf{u},\alpha,\beta)$. When $(\alpha',\beta')$ =  $max_{\alpha,\beta}\mathcal{L}min_u(\mathbf{u},\alpha,\beta)$, **lemma 4** turns out to be true.

**Theorem 4.** Slater Condition: when the primal problem is a convex optimization problem which means $f$ and $g_i$ are convex function and $h_j$ is an affine function. Besides, at least one point in the feasible domain makes the constraints strictly hold. Then, the dual problem is equal to the primal problem

**Theorem 5.** Linear SVM fulfills the Slater Condition.

### 4.2. The dual format of linear SVM

The Lagrange function of linear SVM is
$$\mathcal{L}(\mathbf{w},b,\mathbf{\alpha})=\frac{1}{2}\mathbf{w^\top w}+\sum_{i=1}^m\alpha_i(1-y_i(\mathbf{w^\top x+b}))\tag{19}$$

The dual problem is
$$\underset{\mathbf{\alpha}}{max}\underset{\mathbf{w},b}{min} \quad \frac{1}{2}\mathbf{w^\top w}+\sum_{i=1}^m\alpha_i(1-y_i(\mathbf{w^\top x+b}))\tag{20}$$
$$s.t.\quad \alpha_i\geq0,i=1,2,...,m$$

**Theorem 6.** The dual problem of linear SVM is equal to find a set of parameter $\mathbf{\alpha}$ such that
$$\underset{\alpha}{min} \quad
\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\mathbf{x_i^\top x_j}-\sum_{i=1}^m\alpha_i\tag{21}$$
$$s.t. \quad \sum_{i=1}^m\alpha_iy_i=0, \alpha_i\geq0,i=1,2,...,m$$

Proof:
According to (20), there are no constraints on $\mathbf{w}$ or $b$, so we can calculate the optimal $\mathbf{w}$ and $b$ by letting gradients equal to 0:

$$\frac{\partial\mathcal{L}}{\partial\mathbf{w}}=0\rightarrow\mathbf{w}=\sum_{i=1}^m\alpha_iy_i\mathbf{x}_i$$
$$\frac{\partial\mathcal{L}}{\partial b}=0\rightarrow b=\sum_{i=1}^m\alpha_iy_i=0$$

Plug into (20), we will get the form in **theorem 6.**

**Theorem 7.** The dual format of the linear SVM which is (21) is a convex quadratic optimization problem including $m$ variables and $m+2$ constraints.

Proof: Let
$$\mathbf{u}=\mathbf{\alpha}, \mathbf{Q}=[y_iy_j\mathbf{x_i^\top x_j}]_{m\times m},\mathbf{t}=-1$$
$$\mathbf{c}_i=\mathbf{e}_i, d_i=0,i=1,2,...,m$$
$$\mathbf{c}_{m+1}=[y_1y_2...y_m]^\top,d_{m+1}=0$$
$$\mathbf{c}_{m+2}=-[y_1y_2...y_m]^\top,d_{m+2}=0$$
where $\mathbf{e_i}$ is the vector whose $ith$ position is 1, the rest is 0.
Plug all of them into (8).

### 4.3. Support Vectors

**Theorem 8.** According to **definition 4**, the KKT conditions of the linear SVM are:

* $1- y_i(\mathbf{w^\top x}+b)\leq 0$
* $\alpha_i\geq 0$
* complementary slackness: $\alpha_i(1- y_i(\mathbf{w^\top x}+b))=0$

**Definition 6.** Support Vectors are those samples when $\alpha_i > 0$

**Lemma 5.** For linear SVM, Support Vectors are closer to the classification super plane than any other samples, and they are on the boundary of margin.

Proof:
The first item of KKT conditions in **theorem 8** guarantees that the distance between any samples and the classification super plane is larger than or equal to 1. Besides, according to **theorem 8,** $\alpha_i(1- y_i(\mathbf{w^\top x}+b))=0$. When $\alpha_i>0$, $1- y_i(\mathbf{w^\top x}+b) = 0$, that is $y_i(\mathbf{w^\top x}+b)=1$. Therefore, **lemma 5** is correct.

**Theorem 8.** The parameters $(\mathbf{w},b)$ are determined solely by Support Vectors.

Proof: Based on the fact that when $\alpha_i > 0$, samples are Support Vectors,

$$
\begin{align}
\mathbf{w}&=\sum_{i=1}^m\alpha_iy_i\mathbf{x}_i\\
&=\sum_{i:\alpha_i=0}^m0y_i\mathbf{x}_i+\sum_{i:\alpha_i>0}^m\alpha_iy_i\mathbf{x}_i\\
&=\sum_{i\in SV}\alpha_iy_i\mathbf{x}_i
\end{align}
$$
SV stands for the set of Support Vectors. $b$ can be calculated based on the complementary slackness.
$$b=y-\mathbf{w^\top x_i}=y_{sv} - \sum_{i\in SV}\alpha_iy_i\mathbf{x_i^\top x_i}$$

**Theorem 9.** The function of linear SVM can be shown as follows:

$$h(\mathbf{x})=sign(\sum_{i\in SV}\alpha_iy_i\mathbf{x_i^\top x_i}+y_{sv} - \sum_{i\in SV}\alpha_iy_i\mathbf{x_i^\top x_i})\tag{22}$$

## 5. Soft Margin

What we discussed before have assumed that all the samples in the space can be linearly classified into two classes. In reality, on the other hand, this assumption rarely holds. In order to solve this problem, the solution is to allow some mistakes in the classification process.

### 5.1. Linear SVM with Soft Margin.

We add the another item to the target function showing how many samples that we allow to be incorrectly classified.

$$\underset{\mathbf{w},b}{min}\quad \frac{1}{2}\mathbf{w^\top w}+C\sum_{i=1}^m\bigcirc(y_i\neq sign(\mathbf{w^\top x_i}+b))\tag{23}$$

where $\bigcirc$ is an indicator function, $C$ is a parameter adjusting the balance between the classification accuracy and the sensitivity of the model.

The problem is that the indicator function is not continuous or convex making the problem no longer a quadratic optimization problem.

In order to make the target function continuous, we have to introduce a continuous variable describing the same thing. Slack variable $\xi$ is defined as follows:
$$
\xi_i=\left\{
\begin{aligned}
&0,if \quad y_i(\mathbf{w^\top x_i}+b)\geq1\\
&1-y_i(\mathbf{w^\top x_i}+b), otherwise
\end{aligned}
\right.\tag{24}
$$

**Theorem 10.** The linear SVM with Soft Margin is to find a set of parameters $(\mathbf{w},b)$ such that

$$
\underset{\mathbf{w},b,\mathbf{\xi}}{min} \quad \frac{1}{2}\mathbf{w^\top w}+C\sum_{i=1}^m\xi_i\tag{25}
$$
$$
\begin{align}
s.t. &\quad y_i(\mathbf{w^\top x_i}+b) \geq 1-\xi_i,i=1,2,...,m\\
\\
&\quad\xi_i\geq 0, i = 1,2,...,m
\end{align}
$$

When $C$ is large, we hope more samples are classified correctly but the model would be more sensitive to samples that are close to the classification super plane.

Proof: When a sample fulfills the constraint $y_i(\mathbf{w^\top x_i}+b)\geq1, \xi_i$ can be any value as long as it is larger than or equal to 0, the target function would be minimized when $\xi_i = 0$. When a sample does not meet the constraint, $\xi_i\geq1-y_i(\mathbf{w^\top x_i}+b)$, in order to minimize the target function, $\xi_i=1-y_i(\mathbf{w^\top x_i}+b)$

### 5.2. The dual problem of linear SVM with Soft Margin.

**Theorem 11.** The dual problem of linear SVM with Soft Margin is equal to finding a set of parameters $\mathbf{\alpha}$ such that
$$\underset{\mathbf{\alpha}}{min}\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\mathbf{x_i^\top x_i}-\sum_{i=1}^m\alpha_i\tag{26}$$

$$
\begin{align}
s.t.&\quad \sum_{i=1}^m\alpha_iy_i=0,\\
&\quad 0\leq\alpha_i\leq\xi_i,i=1,2,...,m
\end{align}
$$

Proof: The Lagrange function of linear SVM with Soft Margin is
$$
\begin{align}
\mathcal{L}(\mathbf{w},b,\mathbf{\xi},\mathbf{\alpha,\beta})=&\frac{1}{2}\mathbf{w^\top w+C\sum_{i=1}^m\xi_i}\\
&+\sum_{i=1}^m\alpha_i(1-\xi_i-y_i(\mathbf{w^\top x_i}+b))\\
&+\sum_{i=1}^m\beta_i(-\xi_i)\tag{27}
\end{align}
$$
The dual problem is
$$\underset{\mathbf{\alpha,\beta}}{max}\underset{\mathbf{w},b,\mathbf{\xi}}{min}\quad\mathcal{L}(\mathbf{w},b,\mathbf{\xi,\alpha,\beta})\tag{28}$$

$$
\begin{align}
s.t. &\quad \alpha_i\geq0,i=1,2,...,m\\
&\quad \beta_i\geq0,i=1,2,...,m
\end{align}
$$
Similarly to the normal linear SVM, we can get $\mathbf{w}$, $b$, and $\mathbf{\xi}$ by letting the gradients equal to 0
$$
\frac{\partial\mathcal{L}}{\partial\mathbf{w}}=0\rightarrow \mathbf{w}=\sum_{i=1}^m\alpha_iy_i\mathbf{x_i}
$$
$$
\frac{\partial\mathcal{L}}{\partial b} = 0 \rightarrow \sum_{i=1}^m\alpha_iy_i=0
$$
$$
\frac{\partial\mathcal{L}}{\partial\mathbf{\xi}} = 0 \rightarrow \alpha_i+\beta_i = C
$$
Since $\beta_i=C-\alpha_i\geq0$, we can eliminate $\beta_i$ by letting $0\leq\alpha_i\leq C$

The theorem can be proved if we plug into (28)

**Theorem 12.** The dual problem of linear SVM with Soft Margin belongs to the convex quadratic optimization problem including $m$ variables and $2m+2$ constraints.

Proof:
Let
$$
\mathbf{u}=\mathbf{\alpha}, \mathbf{Q}=[y_iy_j\mathbf{x_i^\top x_j}]_{m\times m}, \mathbf{t}=\mathbf{-1}$$
$$
\mathbf{c}_i=\mathbf{e}_i, d_i=0, i=1,2,...,m
$$
$$
\mathbf{c}_i=-\mathbf{e}_i, d_i=-\xi_i,i=m+1,...,2m$$
$$
\mathbf{c}_{2m+1}=[y_1y_2...y_m]^\top,d_{2m+1}=0
$$
$$
\mathbf{c}_{2m+2}=-[y_1y_2...y_m]^\top,d_{2m+2}=0
$$
Plug into (8).

### 5.3. Support Vectors of linear SVM with Soft Margin.

**Theorem 13.** The KKT conditions for linear SVM with Soft Margin are:

* $1-\xi-y_i(\mathbf{w^\top x_i}+b)\leq0, -\xi_i\leq0$
* $\alpha_i\geq0$, $\beta_i\geq0$
* complementary slackness: $\alpha_i(1-\xi_i- y_i(\mathbf{w^\top x_i}+b))=0$, $\beta_i\xi_i=0$

Proof:
Let
$$
\mathbf{u}=\left[
\begin{matrix}
\mathbf{w}\\
b\\
\mathbf{\xi}
\end{matrix}
\right],
$$

$$
g_i(\mathbf{u})=1-\left[
\begin{matrix}
y_i\mathbf{w}\\
y_i\\
\mathbf{e_i}
\end{matrix}
\right]\mathbf{u},i=1,2,...,m
$$

$$
g_i(\mathbf{u})=-\left[
\begin{matrix}
0\\
\mathbf{e_i}
\end{matrix}
\right]\mathbf{u},i=m+1,m+2,...,2m
$$
Plug into **definition 4**.

**Lemma 6.** In linear SVM with Soft Margin, Support Vectors lie within or on the boundary of margin. Note that samples which are incorrectly classified are also Support Vectors.

Proof:
According to the KKT conditions, $\alpha_i(1-\xi_i- y_i(\mathbf{w^\top x_i}+b))=0$, $\beta_i\xi_i=0$ and $\beta_i\xi_i=0$. For Support Vectors which means $\alpha_i>0$, $1-\xi_i- y_i(\mathbf{w^\top x_i}+b)=0$. Besides, based on the proof process of **theorem 11**, $0\leq\alpha_i\leq C$, we have:
* When $0<\alpha<C, \beta_i = C-\alpha_i>0. So, \xi_i=0$, the samples lies on the boundary of margin.
* When $\alpha_i=C, \beta_i=C-\alpha_i=0$. If $\xi_i\leq1$, the sample lies within the margin; if $\xi_i>1$, then the sample is misclassified.

**Theorem 14.** The linear SVM with soft margin is determined only by Support Vectors.

Proof: The proof theory is the same as the normal linear SVM (**theorem 8**)

### 5.4. Hinge Loss

**Lemma 7.** The Slack Variable $\xi_i$ in (24) is equal to

$$\xi_i=max(0, 1-y_i(\mathbf{w^\top x_i}+b))\tag{29}$$

**Theorem 15.** The linear SVM with Soft Margin is equal to

$$\underset{\mathbf{w},b}{min}\quad C\sum_{i=1}^mmax(0,1-y_i(\mathbf{w^\top x_i}+b))+\frac{1}{2}||\mathbf{w}||^2$$

Proof: Based on (25) in **theorem 10**, let $\xi_i=max(0, 1-y_i(\mathbf{w^\top x_i}+b))$.

**Definition 7.** The definition of Hinge Loss is:

$$l(s_i)=max(0, 1-s_i)$$
where $s_i = y_i\hat{y_i}=y_i(\mathbf{w^\top x_i}+b)$

**Theorem 16.** Based on **theorem 15** and **definition 7**, The loss function of linear SVM with Soft Margin is:
$$Loss = C\cdot\sum_{i=1}^ml(s_i)+\frac{1}{2}||\mathbf{w}||^2\tag{30}$$

A popular explanation for this loss function in the machine learning industry is that it is trying to minimize Hinge Loss with **l2 regularization**.

### 5.5. Some Clarifications

Note that formula **(30)** is basically the same as formula **(26)**. Both of them are the final target function for linear SVM with Soft Margin but from different perspectives. Formula **(26)** is derivative from Lagrange function and the dual problem; formula **(30)**, on the other hand, comes directly from the definition of linear SVM with Soft Margin.

Formula **(30)** is easier to interpret in comparison to formula **(26)**, the first item of formula **(30)** is measuring how accurate the model is and the second item controls the complexity of the model by making the parameters small (**l2 regularization**) which is also widely used in other machine learning models.

In terms of looking for the best parameters $(\mathbf{w},b)$, different algorithms are invented for formula **(30)** and formula **(26)** respectively. For example, **Sequential Minimal Optimization (SMO)** algorithm is for formula **(26)** while **Pegasos** algorithm was created for formula **(30)**.
