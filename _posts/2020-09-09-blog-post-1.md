---
title: 'Mathematics behind linear SVM'
date: 2020-09-09
permalink: /posts/2020/09/blog-post-1/
tags:
  - SVM
  - machine learning
---
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

## Introduction
About ten years ago, Support Vector Machine (SVM) was as hot as what deep neural
networks are like today. It was widely used in the areas like computer vision
and natural language processing. Although machine learning research has been
dominated by deep learning, SVM is still worth the time to study because SVM is
a good application of convex optimization which is widely used in other
algorithms and everyday life. Therefore, questions related to SVM are often asked
in the interviews for the machine learning positions to test the basic math skills
of the candidates. On the top of that, SVM is simple and it works well when the
sample size is small. In this article, I will try to derive the math details
behind linear SVM.

## Binary classification
Given a set of data points $$\frac{1}{2}$$
