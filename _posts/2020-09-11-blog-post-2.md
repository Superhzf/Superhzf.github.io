---
title: 'Sequential Minimal Optimization'
date: 2020-09-11
permalink: /posts/2020/09/blog-post-2/
excerpt: "An efficient method to find the optimal parameters for SVM<br/><img src='/images/smo.png'>"
tags:
  - Machine Learning
  - Optimization
  - SVM
---

## 1. Introduction

In [linear SVM article](https://superhzf.github.io/posts/2020/09/blog-post-1/) article, we discussed about how to derive the target function of linear SVM. The next obstacle for linear SVM is how to find the optimal parameters $(\mathbf{w},b)$.

For the formula that is based on the Lagrange function and the dual problem, we can find the best parameters using the traditional quadratic programming methods. However, the downside of this method is that the space complexity would be $O(m^2)$ which is huge thanks to $\mathbf{Q}=[y_iy_j\mathbf{x_i^\top x_j}]_{m\times m}$. Sequential Minimal Optimization (SMO) is an efficient algorithm designed to find the optimal parameters.

## 2. Optimize two $\alpha_i$

The SMO algorithm searches through the feasible region of the dual problem of the linear SVM with Soft Margin and minimize the following loss function:

$$Loss = \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\mathbf{x_i^\top x_j}-\sum_{i=1}^m\alpha_i\tag{1}$$

$$s.t.\quad\sum_{i=1}^m\alpha_iy_i=0,\tag{2}$$
$$0\leq\alpha_i\leq C,i=1,2,...,m\tag{3}$$

The target of linear SVM with Soft Margin is to find all the $\alpha_i$. Thanks to the constraint (2), SMO tries to optimize two $\alpha_i$ at one time. The vantage points to do this are described as follows.

Suppose $\alpha_1$ and $\alpha_2$ are selected. Then we have
$$
y_1\alpha_1+y_2\alpha_2=y_1\alpha_1^{old}+y_2\alpha_2^{old}=\gamma\tag{4}
$$

This confines the optimization to be a line.

* If $y_1=1$, $y_2=-1$, then we have $\alpha_1-\alpha_2=\gamma$
* If $y_1=-1$, $y_2=1$, then we have $\alpha_1-\alpha_2=-\gamma$
* If $y_1=y_2=1$, then we have $\alpha_1+\alpha_2=\gamma$
* If $y_1=y_2=-1$, then we have $\alpha_1+\alpha_2=-\gamma$


Let $s=y_1y_2$, if we multiply (4) by $y_1$,we have
$$\alpha_1=\gamma-s\alpha_2$$

Fixing the other $\alpha_i$, the loss function (1) can be written as
$$
\begin{align}
Loss =
&\frac{1}{2}(y_1y_1\mathbf{x_1^\top x_1}\alpha_1^2+y_2y_2\mathbf{x_2^\top x_2}\alpha_2^2+2y_1y_2\mathbf{x_1^\top x_2}\alpha_1\alpha_2\\
&+2(\sum_{i=3}^m\alpha_iy_i\mathbf{x_i^\top})(y_1\mathbf{x_1}\alpha_1+y_2\mathbf{x_2}\alpha_2)+Const1)\\
&-(\alpha1+\alpha2+Const2)
\end{align}\tag{5}
$$
Note that $Const1$ and $Const2$ are not real constant variables. Instead, $Const1$ and $Const2$ stand for variables that have no relationship with $\alpha_1$ or $\alpha_2$. The same concept works for the following $Const.$

Let $K_{11}=\mathbf{x_1^\top x_1}$, $K_{22}=\mathbf{x_2^\top x_2}$, $K_{12}=\mathbf{x_1^\top x_2}$

Besides, according to the proof process of **theorem 11** in the [linear SVM article](https://superhzf.github.io/posts/2020/09/blog-post-1/), $w=\sum_{i=1}^m\alpha_iy_i\mathbf{x_i}$, we have

$$
\begin{align}
v_j&=\sum_{i=3}^m\alpha_iy_i\mathbf{x_i^\top x_j}\\
&=\mathbf{w^\top_{old} x_j} - \alpha_1^{old}y_1\mathbf{x_1^\top x_j} -\alpha_2^{old}y_2\mathbf{x_2\top x_j}\\
&=(\mathbf{w^\top_{old} x_j}-b^{old}) + b^{old}-\alpha_1^{old}y_1\mathbf{x_1^\top x_j} -\alpha_2^{old}y_2\mathbf{x_2\top x_j}\\
&=u^{old}_j+b^{old}-\alpha_1^{old}y_1\mathbf{x_1^\top x_j} -\alpha_2^{old}y_2\mathbf{x_2\top x_j}
\end{align}
$$

where $u^{old}_j=\mathbf{w^\top_{old} x_j}-b^{old}$ is the output of $\mathbf{x_j}$ under old parameters.

$$
\begin{align}
Loss &= \frac{1}{2}(K_{11}\alpha_1^2+K_{22}\alpha_2^2+2sK_{12}\alpha_1\alpha_2+2y_1v_1\alpha_1+2y_2v_2\alpha_2)-\alpha1-\alpha2-Const.\\
&=\frac{1}{2}(K_{11}(\gamma-s\alpha_2)^2+K_{22}\alpha_2^2+2sK_{12}(\gamma-s\alpha_2)\alpha_2+2y_1v_1(\gamma-s\alpha2)+2y_2v_2\alpha_2)-\gamma+s\alpha_2-\alpha_2+Const.\\
&=\frac{1}{2}K_{11}(\gamma-s\alpha_2)^2+\frac{1}{2}K_{22}\alpha_2^2+sK_{12}(\gamma-s\alpha_2)\alpha_2y+y_1v_1(\gamma-s\alpha_2)+y_2v_2\alpha_2-(1-s)\alpha_2+Const.\\
&=-\frac{1}{2}(2K_{12}-K_{11}-K_{22})\alpha_2^2-(1-s+sK_{11}\gamma-sK_{12}\gamma+y_2v_1-y_2v_2)\alpha_2+Const.
\end{align}
$$

Let $\eta=2K_{12}-K_{11}-K_{12}$. The coefficient of $\alpha_2$ is

$$
\begin{align}
&1-s+sK_{11}\gamma-sK_{12}\gamma+y_2v_1-y_2v_2\\
=&1-s+sK_{11}(\alpha^{old}_1+s\alpha^{old}_2)-sK_{12}(\alpha^{old}_1+s\alpha^{old}_2)+\\
&y_2(u_1^{old}+b^{old}-\alpha_1^{old}y_1K_{11}-\alpha_2^{old}y_2K_{12})\\
&-y_2(u_2^{old}+b^{old}-\alpha_1^{old}y_1K_{12}-\alpha_2^{old}y_2K_{22})\\
=&1-s+(sK_{11}-sK_{12}-sK_{11}+sK_{12})\alpha_1^{old}+\\
&(K_{11}-2K_{12}+K_{22})\alpha_2^{old} + y_2(u_1^{old}+u_2^{old})\\
=&y_2^2-y_1y_2+(K_{11}-2K_{12}+K_{22})\alpha_2^{old}+y_2(u_1^{old}-u_2^{top})\\
=&y_2(y_2-y_1+u_1^{old}-u_2^{old})-\eta\alpha_2^{old}\\
=&y_2((u_1^{old}-y_1)-(u_2^{old}-y_2))-\eta\alpha_2^{old}\\
=&y_2(E_1^{old}-E_2^{old})-\eta\alpha_2^{old}
\end{align}
$$
Where $E_i^{old}=u_i^{old}-y_i=\mathbf{w^\top_{old}x_i}-b^{old}-y_i\tag{6}$

So, the loss function is:
$$Loss = (\eta\alpha_2^{old}-y_2(E_1^{old}-E_2^{old}))\alpha_2-\frac{1}{2}\eta\alpha_2^2+Const.$$

The first and second derivatives are:
$$\frac{\partial Loss}{\partial\alpha_2}=\eta\alpha_2^{old}-y_2(E_1^{old}-E_2^{old})-\eta\alpha_2,$$
$$\frac{\partial^2Loss}{\partial\alpha_2^2}=\eta$$

Let $\frac{\partial Loss}{\partial\alpha_2}=0$,

$$\alpha_2^{new}=\alpha_2^{old}-\frac{y_2(E_1^{old}-E_2^{old})}{\eta}\tag{7}$$

**Lemma 1.** $\eta=2K_{12}-K_{11}-K_{22}\leq0$.

Proof: Let $K_{11}=\mathbf{x_1^\top x_1}$, $K_{12}=\mathbf{x_1^\top x_2}$, $K_{22}=\mathbf{x_2^\top x_2}$. Then $\eta=-(\mathbf{x_2-x_1})^\top(\mathbf{x_2-x_1})=-\|\mathbf{x_2-x_1\|}^2\leq0$

If $\eta<0$, formula (6) gives us the unconstrained maximum point $\alpha_2^{new}$. It must be checked against the feasible range. Let $s=y_1y_2$, and $\gamma=\alpha_1^{old}+s\alpha_2^{old}$. The range of $\alpha_2$ is determined as follows:

* If $s=1$, then $\alpha_1+\alpha_2=\gamma$.
  - If $\gamma>C$, then max $\alpha_2=C$, and min $\alpha_2=\gamma -C$
  - If $\gamma<C$, then max $\alpha_2 = \gamma$, and min $\alpha_2=0$

* If $s = -1$, then $\alpha_1-\alpha_2=\gamma$.
  - If $\gamma > 0$, then max $\alpha_2=C-\gamma$, and min $\alpha_2=C-\gamma$
  - If $\gamma < 0$, then max $\alpha_2 = C$, and min $\alpha_2=-\gamma$.

  Let the minimum feasible value of $\alpha_2$ be $L$, maximum be $H$. Then

$$
\alpha_2^{new, clipped}=\left\{
\begin{aligned}
H,&\quad if\quad H<\alpha_2^{new},\\
\alpha_2^{new},&\quad if\quad L\leq\alpha_2^{new}\leq H,\\
L,&\quad if \quad\alpha_2<L.
\end{aligned}
\right.\tag{8}
$$

To summarize, given $\alpha_1$ and $\alpha_2$ (and corresponding $y_1$, $y_2$, $K_{11}$, $K_{12}$, $K_{22}$, and $E_1^{old}-E_2^{old}$), we can optimize the two $\alpha$ by the following procedure:

1. $\eta=2K_{12}-K_{11}-K_{22}$

2. If $\eta<0$,
$$\alpha_2^{new}=\alpha_2^{old}-\frac{y_2(E_1^{old}-E_2^{old})}{\eta}$$
and clip it based on formula (8)

3. Based on the formula (4), $\Delta\alpha_1=-s\Delta\alpha_2$

4. If $\eta=0$, we need to evaluate the objective function at the two endpoints, and set $\alpha_2^{new}$ to be the one with the minimum loss function value. The loss function is
$$Loss = (\eta\alpha_2^{old}-y_2(E_1^{old}-E_2^{old}))\alpha_2-\frac{1}{2}\eta\alpha_2^2+Const.\tag{8}$$

## 3. Updating after a successful optimization step

The final destination of the entire optimization is to find a set of parameters $(\mathbf{w},b)$. Therefore, after updating $\alpha_1$ and $\alpha_2$, it is time to update $\mathbf{w}$ and $b$.

### 3.1. Update $b$

According to the formula (6), we know the definition of $E_i$. Let $E(\mathbf{x},y)$ be the prediction error on $(\mathbf{x},y)$:

$$
E(\mathbf{x},y)=(\sum_{i=1}^m\alpha_i y_i\mathbf{x_i}^\top)\mathbf{x}-b-\mathbf{y}\tag{9}
$$

Based on the formula (9) and the updated $\alpha_1$ and $\alpha_2$, we have the following $E^{new}(\mathbf{x},y)$ and $E^{old}(\mathbf{x},y)$

$$
E^{new}(\mathbf{x},y)=(\sum_{i=3}^m\alpha_i y_i\mathbf{x}_i^\top+\alpha_1^{new}y_1\mathbf{x}_1^\top+\alpha_2^{new}y_2\mathbf{x}_2^\top)\mathbf{x}-b^{new}-y\tag{10}
$$

$$
E^{old}(\mathbf{x},y)=(\sum_{i=3}^m\alpha_i y_i\mathbf{x}_i^\top+\alpha_1^{old}y_1\mathbf{x}_1^\top+\alpha_2^{old}y_2\mathbf{x}_2^\top)\mathbf{x}-b^{old}-y\tag{11}
$$

If we let formula (10) - formula (11):

$$
\begin{align}
&E^{new}(\mathbf{x},y)-E^{old}(\mathbf{x},y)\\
=&\Delta\alpha_1y_1\mathbf{x^\top_1}+\Delta\alpha_2y_2\mathbf{x^\top_2}-\Delta b
\end{align}
$$
So, we have:
$$\Delta b=\Delta\alpha_1y_1\mathbf{x^\top_1}+\Delta\alpha_2y_2\mathbf{x^\top_2}+E^{old}(\mathbf{x},y)-E^{new}(\mathbf{x},y)\tag{12}$$

$$b^{new}=b^{old}+\Delta b\tag{13}$$

### 3.2. Update $\mathbf{w}$

Besides, according to the proof process of **theorem 11** in the [linear SVM article](https://superhzf.github.io/posts/2020/09/blog-post-1/),
$$w=\sum_{i=1}^m\alpha_iy_i\mathbf{x_i}$$
Thus,
$$
\begin{align}
w^{new}&=\sum_{i=1}^m\alpha_iy_i\mathbf{x_i}\\
&=\sum_{i=3}^m\alpha_i y_i\mathbf{x}_i + \alpha_1^{new}y_1\mathbf{x}_1+\alpha_2^{new}y_2\mathbf{x}_2
\end{align}\tag{14}
$$
and
$$
\begin{align}
w^{old}&=\sum_{i=1}^m\alpha_iy_i\mathbf{x_i}\\
&=\sum_{i=3}^m\alpha_i y_i\mathbf{x}_i + \alpha_1^{old}y_1\mathbf{x}_1+\alpha_2^{old}y_2\mathbf{x}_2
\end{align}\tag{15}
$$
If we let formula (14) - formula (15):
$$\Delta\mathbf{w}=\Delta\alpha_1y_1\mathbf{x_1}+\Delta\alpha_2y_2\mathbf{x_2}\tag{15}$$

$$\mathbf{w}^{new} = \mathbf{w}^{old}+\Delta\mathbf{w}\tag{16}$$

## 4. How to pick two $\alpha_i$ for optimization

It is said that two $\alpha_i$ can be selected randomly. Based on my experience, on the other hand, random selecting $\alpha_i$ leads to a poor performance.

The strategy used by the original paper is

* The outer loop selects the first $\alpha_i$, the inner loop selects the second $\alpha_i$ that maximizes $\mid E_1-E_2\mid$

* The outer loop alternates between one sweep through all examples and as many sweeps as possible through non-boundary examples (those with $0<\alpha_i<C$), selecting the examples that violates the KKT condition.

* Given the first $\alpha_i$, the inner loop looks for a non-boundary sample that maximizes $\mid E_2-E_1\mid$. If this does not make progress, it starts a sequential scan through all the samples.

Based on my experience, it still works well if I choose the second $\alpha_i$ which has the maximum prediction error instead of the max $\mid E_1-E_2\mid$.

Appendix:

[Sample SMO code at Github](https://github.com/Superhzf/MLFromScratch/blob/master/numpy_ml/supervised_learning/support_vector_machine.py)
<p align="center">
<img src="/images/sample_smo.png">
</p>
<p align="center">
    The result of the sample code
</p>
